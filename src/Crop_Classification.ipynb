{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7GOR6LkIgmj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ],
   "metadata": {
    "id": "NEkA7Fn2X_18"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_paths, transform=None):\n",
    "        self.data_paths = data_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        labels = [int(label) for label in img_path.stem.split('_')[-3:]]\n",
    "\n",
    "        return image, torch.tensor(labels)"
   ],
   "metadata": {
    "id": "jn6nago8JShT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        self.conv3 = nn.Conv2d(96, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        #\n",
    "        # self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        # self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        # self.bn5 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Adjust the input size for the fully connected layer\n",
    "        self.fc1 = nn.Linear(64 * (input_size // 4) * (input_size // 4), hidden_size)\n",
    "        # self.fc1 = nn.Linear(98304, hidden_size)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc5 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # x = F.relu(self.conv3(x))\n",
    "        # x = self.bn3(x)\n",
    "        # x = self.pool(x)\n",
    "\n",
    "        # x = F.relu(self.conv4(x))\n",
    "        # x = self.bn4(x)\n",
    "        # x = self.pool(x)\n",
    "\n",
    "        # x = F.relu(self.conv5(x))\n",
    "        # x = self.bn5(x)\n",
    "        # x = self.pool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn_fc1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # x = F.relu(self.fc4(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "id": "30rmhR3yJUGD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# dataset_path = \"/content/drive/Othercomputers/My Laptop/objective_2_Crop_classification\"\\\n",
    "dataset_path = \"../input/objective_2_crop_classification\"\n",
    "data_paths = list(Path(dataset_path).rglob(\"*.tif\"))\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_paths, test_paths = train_test_split(data_paths, test_size=0.2, random_state=42)\n",
    "train_paths, val_paths = train_test_split(train_paths, test_size=0.2, random_state=42)\n"
   ],
   "metadata": {
    "id": "I52dFhszJWvT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = CustomDataset(train_paths, transform=transform)\n",
    "val_dataset = CustomDataset(val_paths, transform=transform)\n",
    "test_dataset = CustomDataset(test_paths, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "id": "xsggnkBvJZ3-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_size = 64\n",
    "hidden_size = 128\n",
    "num_classes = 15\n",
    "input_channels = 3\n",
    "model = NeuralNetwork(input_channels, input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n"
   ],
   "metadata": {
    "id": "ZdhzU86vWREz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Start training\")\n",
    "\n",
    "num_epochs = 50\n",
    "test_every_epochs = 1\n",
    "\n",
    "has_checkpoint = False\n",
    "clip_value = 0.01\n",
    "\n",
    "if has_checkpoint:\n",
    "    checkpoint = torch.load('saved_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    starting_epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "else:\n",
    "    starting_epoch = 0\n",
    "\n",
    "for epoch in range(starting_epoch, starting_epoch + num_epochs):\n",
    "    model.train()\n",
    "    # training_percentage = random.uniform(0.6, 0.8)\n",
    "    # sampled_indices = random.sample(range(len(train_dataset)), int(training_percentage * len(train_dataset)))\n",
    "    # sampled_train_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(train_dataset, sampled_indices), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels[:, -1])\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Perform validation every epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_top1 = 0\n",
    "        correct_top3 = 0\n",
    "        total = 0\n",
    "        val_loss = 0\n",
    "        y_true_val = []\n",
    "        y_score_val = []\n",
    "\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.topk(outputs, k=3, dim=1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                if labels[i, -1] in predicted[i]:\n",
    "                    correct_top3 += 1\n",
    "                    if labels[i, -1] == predicted[i, 0]:\n",
    "                        correct_top1 += 1\n",
    "\n",
    "            y_true_val.extend(labels[:, -1].cpu().numpy())\n",
    "            y_score_val.extend(outputs.cpu().numpy())\n",
    "            val_loss += criterion(outputs, labels[:, -1]).item()\n",
    "\n",
    "        accuracy_top1 = correct_top1 / total\n",
    "        accuracy_top3 = correct_top3 / total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        f1_val = f1_score(y_true_val, np.argmax(y_score_val, axis=1), average='weighted')\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{starting_epoch + num_epochs}, \"\n",
    "              f\"Validation Top-1 Accuracy: {accuracy_top1 * 100:.2f}%, \"\n",
    "              f\"Top-3 Accuracy: {accuracy_top3 * 100:.2f}%, \"\n",
    "              f\"Validation F1 Score: {f1_val:.4f}, \"\n",
    "              f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % test_every_epochs == 0 or epoch == num_epochs - 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct_top1 = 0\n",
    "            correct_top3 = 0\n",
    "            total = 0\n",
    "            test_loss = 0\n",
    "\n",
    "            y_true_test = []\n",
    "            y_score_test = []\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.topk(outputs, k=3, dim=1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "\n",
    "                for i in range(labels.size(0)):\n",
    "                    if labels[i, -1] in predicted[i]:\n",
    "                        correct_top3 += 1\n",
    "                        if labels[i, -1] == predicted[i, 0]:\n",
    "                            correct_top1 += 1\n",
    "\n",
    "                y_true_test.extend(labels[:, -1].cpu().numpy())\n",
    "                y_score_test.extend(outputs.cpu().numpy())\n",
    "\n",
    "                test_loss += criterion(outputs, labels[:, -1]).item()\n",
    "\n",
    "            accuracy_top1 = correct_top1 / total\n",
    "            accuracy_top3 = correct_top3 / total\n",
    "            avg_test_loss = test_loss / len(test_loader)\n",
    "            f1_val = f1_score(y_true_test, np.argmax(y_score_test, axis=1), average='weighted')\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{starting_epoch + num_epochs}, \"\n",
    "                  f\"Test Top-1 Accuracy: {accuracy_top1 * 100:.2f}%, \"\n",
    "                  f\"Top-3 Accuracy: {accuracy_top3 * 100:.2f}%, \"\n",
    "                  f\"Test F1 Score: {f1_val:.4f}, \"\n",
    "                  f\"Test Loss: {avg_test_loss:.4f}\"\n",
    "                  )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CSH4UyhWWS21",
    "outputId": "7a029f51-e054-46cc-99ef-3e9ec30bc59d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "checkpoint = {\n",
    "    'epoch': epoch + 1,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss.item(),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'saved_model.pth')"
   ],
   "metadata": {
    "id": "Kzb9YWSovg_P"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}